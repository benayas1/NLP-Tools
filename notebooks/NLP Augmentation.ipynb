{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "from gensim.test.utils import common_texts\n",
    "import gensim\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..')))\n",
    "import nlp.sources.bigquery as nlpbq\n",
    "import nlp.sources.data_sources as nlpcsv\n",
    "import nlp.integration as nlpint\n",
    "import nlp.augmentation as nlpaugmentation\n",
    "from nlp.pt.dataset import TextDataset, DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Please could you explain my bill to me',\n",
    "             'hi can you expalin what these extra charges are on my bill please £241.73 ???',\n",
    "             'Hi I was just chatting with Saif but got cut off is he available?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translator = nlpaugmentation.BackTranslation(language='de')\n",
    "back_translator.augment(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonyms Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = np.concatenate([pd.read_csv('input/data_triage.csv')['text'].values, pd.read_csv('input/data_upgrade.csv')['text'].values])\n",
    "data = [ [x for x in re.sub( r'[\\d]*','',re.sub(r'[,!?;-]+', '', str(s).lower().replace('.',''))).split(' ') if x!=''] for s in data]\n",
    "model = gensim.models.Word2Vec(sentences=data, vector_size=300, window=10, min_count=1, workers=4)\n",
    "model.wv.save_word2vec_format(\"input/word2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = nlpaugmentation.SynonymReplacement(embeddings_path='input/word2vec.bin', model_type='word2vec')\n",
    "synonyms.augment(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonyms Replacement Fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a W2V model on 496279 texts\n",
      "Wall time: 57.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "synonyms_fast = nlpaugmentation.SynonymReplacementFast(data= np.concatenate([pd.read_csv('input/data_triage.csv')['text'].values, pd.read_csv('input/data_upgrade.csv')['text'].values]),\n",
    "                                                       device='cpu', \n",
    "                                                       top_k=4, \n",
    "                                                       size=300,\n",
    "                                                       aug_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:23<00:00,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in tqdm(range(100)):\n",
    "    batch = sentences * 10\n",
    "    synonyms_fast.augment(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneof = nlpaugmentation.OneOf([back_translator, synonyms], [0.75])\n",
    "oneof.augment(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = nlpaugmentation.Sequence([back_translator, synonyms])\n",
    "seq.augment(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenter within DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "sentences = pd.read_csv('input/data_triage.csv')['text'].values[:4]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "dataset = TextDataset(data=np.array(sentences),\n",
    "                      labels=np.zeros(len(sentences)),\n",
    "                      class_weights='auto',\n",
    "                      device='cpu',\n",
    "                      only_labelled=True)\n",
    "collate_fn = DataCollator(tokenizer, nlp = None, tag2id = None, ner=False, max_length=40, augmenter = synonyms_fast)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=16, collate_fn=collate_fn, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(train_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(x['x']['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
